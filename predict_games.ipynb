{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = pd.read_csv('preprocess_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "game_data = game_data.sort_values(by=['TEAM_ID_home', 'GAME_DATE_home'])\n",
    "\n",
    "game_data.drop(columns=[\n",
    "    'SEASON_ID_home', 'TEAM_ID_home', 'TEAM_ABBREVIATION_home', \n",
    "    'TEAM_NAME_home', 'GAME_ID', 'GAME_DATE_home'\n",
    "], inplace=True)\n",
    "\n",
    "# , 'MATCHUP_home', \n",
    "#     'WL_home', 'home_away_home', 'SEASON_ID_away', 'TEAM_ID_away', \n",
    "#     'TEAM_ABBREVIATION_away', 'TEAM_NAME_away', 'GAME_DATE_away', \n",
    "#     'MATCHUP_away', 'WL_away', 'home_away_away'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = game_data.dropna()\n",
    "data = data[9:]\n",
    "\n",
    "X = data.drop(columns=['WL'])\n",
    "y = data['WL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LassoCV(cv=5, random_state=42, max_iter=5000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best alpha using built-in LassoCV:\", pipeline.named_steps['model'].alpha_)\n",
    "\n",
    "# Display coefficients\n",
    "lasso_coefficients = pd.Series(pipeline.named_steps['model'].coef_, index=game_data.drop(columns=['WL']).columns)\n",
    "print(\"LASSO Coefficients:\\n\", lasso_coefficients.sort_values())\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "lasso_coefficients.plot(kind='bar')\n",
    "plt.title('Feature Coefficients from LASSO')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features with non-zero coefficients\n",
    "selected_features = lasso_coefficients[lasso_coefficients != 0].index\n",
    "#print(\"Selected Features:\\n\", selected_features)\n",
    "\n",
    "selected_features = selected_features.to_list()\n",
    "selected_features.append('WL')\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = game_data[selected_features]\n",
    "lasso = lasso[9:]\n",
    "lasso = lasso.dropna()\n",
    "\n",
    "X = lasso.drop(columns=['WL'])\n",
    "y = lasso['WL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_coefficients(pipeline, feature_names):\n",
    "    coefficients = pipeline.named_steps['model'].coef_[0]\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "    return coef_df.sort_values('Coefficient', ascending=False)\n",
    "\n",
    "display_coefficients(pipeline, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am just going to try a model with this and just see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = game_data.sort_values(by=['TEAM_ID_home', 'GAME_DATE_home'])\n",
    "game_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data.drop(columns=[\n",
    "    'SEASON_ID_home', 'TEAM_ID_home', 'TEAM_ABBREVIATION_home', \n",
    "    'TEAM_NAME_home', 'GAME_ID', 'GAME_DATE_home', 'Unnamed: 0'\n",
    "], inplace=True)\n",
    "\n",
    "# , 'MATCHUP_home', \n",
    "#     'WL_home', 'home_away_home', 'SEASON_ID_away', 'TEAM_ID_away', \n",
    "#     'TEAM_ABBREVIATION_away', 'TEAM_NAME_away', 'GAME_DATE_away', \n",
    "#     'MATCHUP_away', 'WL_away', 'home_away_away'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = game_data[[\n",
    "    'cumulative_FGM_diff', 'cumulative_FGA_diff',\n",
    "       'cumulative_FG3M_diff', 'cumulative_FG3A_diff', 'cumulative_FTM_diff',\n",
    "       'cumulative_FTA_diff', 'cumulative_OREB_diff', 'cumulative_DREB_diff',\n",
    "       'cumulative_REB_diff', 'cumulative_AST_diff', 'cumulative_STL_diff',\n",
    "       'cumulative_BLK_diff', 'cumulative_TOV_diff', 'cumulative_PTS_diff',\n",
    "       'cumulative_FG_PCT_diff', 'cumulative_FG3_PCT_diff',\n",
    "       'cumulative_FT_PCT_diff', 'cumulative_AST_TOV_diff',\n",
    "       'cumulative_REB_PCT_diff',\n",
    "    'WL'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data = data[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['WL'])\n",
    "y = data['WL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_coefficients(pipeline, feature_names):\n",
    "    coefficients = pipeline.named_steps['model'].coef_[0]\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "    return coef_df.sort_values('Coefficient', ascending=False)\n",
    "\n",
    "display_coefficients(pipeline, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature importance plot\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort and plot\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation=45)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "importances = gb.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort and plot\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation=45)\n",
    "plt.title(\"Feature Importances (Gradient Boosting)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
